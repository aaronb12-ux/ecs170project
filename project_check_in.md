# Core Questions and Group Contributions
This project, a browser extension that is designed to compare text between job listings and a user’s resume using large language models (LLMs), is currently in a research and proof of concept stage. Progress so far has been divided into three main areas, which our team handled in three subgroups. The subgroups include research into the baseline (non-ML) method of similarity score generation, research into the AI methods of similarity score and resume sentence generation, and proof of concept for the frontend browser implementation. 

Group members Anish, Aaron, and Aiperi completed research and a proof of concept to make sure a browser extension can be built for this project. At this stage, the extension can read information from a job posting specifically on Indeed. This may eliminate our need to implement web scraping in our backend. Once this browser extension can be connected to the backend implementation of the baseline and AI methods, the extension can be turned into a polished project for presentation. 

Group members Joseph, Josh, Alexis, and Nedaa conducted research regarding different baseline methods that do not use AI models. This research explored general sentiment analysis techniques, including VADER (Valence Aware Dictionary and sEntiment Reasoner) and SentiWordNet. The team found that these sentiment analysis techniques are difficult to apply to this project, since sentiment analysis mainly focuses on emotions and opinions. Typical applications of sentiment analysis include identifying opinions on subjects through user-generated reviews, whereas this project mainly relies on matching documents based on semantic similarity. In the next stage of this project, one baseline method we plan to implement is a form of word lookup between a job description and a resume. Two lists of unique, non-filler words from a job description and from a user’s resume will be generated, respectively, and a similarity score based on the number of shared words between the two files will be computed. 

Group members Zeeva, Aaron, and Molly have focused on AI-based generation of general similarity scores between the resume and job description, as well as the identification of highly similar sentences between the resume and job description. These were implemented using the BERT and Word2Vec models by generating vector embeddings for each file, as well as the sentences of each file. Then, cosine similarity was calculated to create a similarity score for the two files and all sentences between the two files, though only sentence pairs with a similarity score above 50 were presented. Based on the project proposal feedback, the AI methods we will use have evolved to integrate an autoregressive model for language generation tasks, in two potential workflows. The first workflow option is to input the vector embeddings or other outputs of BERT and Word2Vec into an LLM, either a GPT or Llama model. The goal of using LLM models will be to generate replacement sentences for a user’s resume, based on the job description. Replacement sentences will be generated only for highly similar sentence pairs between the resume and job description, likely based on computed similarity scores between sentences, for the user to utilize for ATS optimization purposes. The second workflow option is to use the BART model, which includes both BERT and autoregressive capabilities. Further research will need to be done regarding how to prompt this model.

One challenge the group has encountered is in regards to the frontend of the browser extension. We have discussed during our weekly team meetings whether to implement all the backend logic on the frontend within the extension using JavaScript (JS), running the model locally, or have a server running in Python that would execute in the backend, connected through an API. Once more of our backend is complete, we will experiment with each method to determine the difficulty of either converting the existing Python code to JS versus implementing an API. Another potential roadblock we might face is getting accurate replacement sentences for the resumes to apply directly to the job description, which might be combated by comparing and combining different models to generate accurate responses. However, we will also need to research what potential metrics we can use to evaluate generated language.

# Reflection
In terms of the group’s experiences while working on this project, we have better understood how natural language processing (NLP) tasks can range from simple keyword analysis to complex semantic understanding. At first, we assumed that using large AI models was always necessary for text comparison, but we’ve learned that there are many non-ML methods used for sentiment analysis. This has helped us understand that we shouldn’t necessarily focus on using the largest model available, but instead on choosing the right method for our task. We have also realized the importance of preprocessing (cleaning text, removing filler words, and identifying key phrases) our text data for this task. 


# Questions and Requests for Feedback
One area we’d like to request feedback on is the comparison of converting the Python code with our AI model integration to JS versus using an API implementation that would keep our Python code intact. Another area we have questions about is regarding best practices on prompting LLMs and particularly how to use vector embeddings as inputs to autoregressive models for language generation. Additionally, we’d like to ask about recommended evaluation metrics (beyond our computed similarity scores) for comparing our baseline and ML methods, as well as for evaluating LLM generated text output. 


# Remaining Work Division
Remaining work for this project includes making our AI implementations more robust, in integrating BERT’s output (embedded vectors) into an LLM for text generation, as well as implementing BART for combined similarity scoring and text generation. This will be done by group members Zeeva, Aaron, Molly, Anish, and Aiperi. Research regarding how to incorporate the ML methods in the backend with the browser extension frontend will be done by group member Aaron. Implementation of some of the researched baseline methods will be done by group members Joseph, Josh, Alexis, and Nedaa. 











